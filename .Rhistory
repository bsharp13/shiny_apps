library(caret)
y <- rbinom(1000, 1, 0.1)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x)
df <- data_frame(y, x1)
for (i in 1:99) {
x <- rnorm(1000, 0, 1)
df <- cbind(df, x)
}
colnames(df) <- c(y, paste0('x', 1:100))
c(y, paste0('x', 1:100))
paste0('x', 1:100)
c(y, paste0('x', 1:100))
colnames(df)[-1] <- paste0('x', 1:100)
set.seed(314)
y <- rbinom(1000, 1, 0.1)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x1)
for (i in 1:99) {
empty <- rbinom(1, 1, 0.05)
if (empty) {
x <- rep(0, 1000)
} else {
x <- rep(1000, 0, 1)
}
df <- cbind(df, x)
}
colnames(df)[-1] <- paste0('x', 1:100)
sums <- colSums(df)
sums
set.seed(314)
y <- rbinom(1000, 1, 0.1)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x1)
for (i in 1:99) {
empty <- rbinom(1, 1, 0.05)
if (empty) {
x <- rep(0, 1000)
} else {
x <- rnorm(1000, 0, 1)
}
df <- cbind(df, x)
}
colnames(df)[-1] <- paste0('x', 1:100)
sums <- colSums(df)
sums
zero_cols <- names(colSums(df)[which(colSums(df) == 0)]
zero_cols <- names(colSums(df)[which(colSums(df) == 0)])
df %>% select(-c(zero_cols))
df %<>% select(-c(zero_cols))
library(magrittr)
df %<>% select(-c(zero_cols))
# Initial Model Selection
#------------------------------------------------------------------------------
nzv <- nearZeroVar(mdrrDescr, saveMetrics= TRUE)
# Initial Model Selection
#------------------------------------------------------------------------------
nzv <- nearZeroVar(df, saveMetrics= TRUE)
View(nzv)
set.seed(314)
y <- rbinom(1000, 1, 0.05)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x1)
for (i in 1:99) {
empty <- rbinom(1, 1, 0.05)
if (empty) {
x <- rep(0, 1000)
} else {
x <- rnorm(1000, 0, 1)
}
df <- cbind(df, x)
}
colnames(df)[-1] <- paste0('x', 1:100)
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE)
View(nzv)
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
filter(nzv)
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
filter(nzv) %>%
row.names()
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
filter(nzv) #%>%
View(nzv)
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) #%>%
View(nzv)
# Remove Empty Columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
rownames_to_column('variable') %>%
filter(nzv) %>%
pull(variable)
# Find Highly Correlated Predictors
df_cor <- cor(df)
# Find Highly Correlated Predictors
mod_x <- df %>% select(-y)
df_cor <- cor(mod_x)
df %<>% select(-c(nzv))
# Find Highly Correlated Predictors
mod_x <- df %>% select(-y)
df_cor <- cor(mod_x)
highly_cor <- findCorrelation(df_cor, cutoff = .80)
df_cor <- cor(mod_x)
highly_cor <- findCorrelation(df_cor, cutoff = .80)
# Filter out linear dependencies
linear_combo <- findLinearCombos(mod_x)
# Filter out linear dependencies
linear_combo <- findLinearCombos(mod_x)$remove
?transparentTheme()
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
# Perform PCA
df %<>% preProcess(method = 'pca')
set.seed(314)
y <- rbinom(1000, 1, 0.05)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x1)
for (i in 1:99) {
empty <- rbinom(1, 1, 0.05)
if (empty) {
x <- rep(0, 1000)
} else {
x <- rnorm(1000, 0, 1)
}
df <- cbind(df, x)
}
colnames(df)[-1] <- paste0('x', 1:100)
# Remove empty columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
rownames_to_column('variable') %>%
filter(nzv) %>%
pull(variable)
df %<>% select(-c(nzv))
# Find highly correlated predictors
mod_x <- df %>% select(-y)
highly_cor <- findCorrelation(cor(mod_x), cutoff = .80)
mod_x %<>% select(-c(highly_cor))
df %<>% select(-c(highly_cor))
# Filter out linear dependencies
linear_combo <- findLinearCombos(mod_x)$remove
mod_x %<>% select(-c(linear_combo))
df %<>% select(-c(linear_combo))
# Perform PCA
df_pre <- df %>% preProcess(method = 'pca')
# Perform PCA
pca_process <- df %>% preProcess(method = 'pca')
df_clean <- predict(pca_process, df)
View(df_clean)
# Partition data
#------------------------------------------------------------------------------
train_rows <- createDataPartition(df_clean$y, p = 0.6, list = FALSE)
set.seed(314)
y <- rbinom(1000, 1, 0.05)
x1 <- rnorm(1000, 0, 1)
df <- data_frame(y, x1)
for (i in 1:99) {
empty <- rbinom(1, 1, 0.05)
if (empty) {
x <- rep(0, 1000)
} else {
x <- rnorm(1000, 0, 1)
}
df <- cbind(df, x)
}
colnames(df)[-1] <- paste0('x', 1:100)
# Remove empty columns
nzv <- nearZeroVar(df, saveMetrics= TRUE) %>%
rownames_to_column('variable') %>%
filter(nzv) %>%
pull(variable)
df %<>% select(-c(nzv))
# Find highly correlated predictors
mod_x <- df %>% select(-y)
highly_cor <- findCorrelation(cor(mod_x), cutoff = .80)
mod_x %<>% select(-c(highly_cor))
# Filter out linear dependencies
linear_combo <- findLinearCombos(mod_x)$remove
mod_x %<>% select(-c(linear_combo))
# Perform PCA
pca_process <- mod_x %>% preProcess(method = 'pca')
mod_x <- predict(pca_process, mod_x)
mod_y <- df$y
# Partition data
#------------------------------------------------------------------------------
train_rows <- createDataPartition(mod_y, p = 0.6, list = FALSE)
?train()
mod_y <- as.factor(df$y)
# Algorithm Selection
#------------------------------------------------------------------------------
# Define custom train function
my_train <- function(method) {
mod <- train(
x = train_x,
y = train_y,
method = method,
tuneLength = 5,
metric = 'kappa',
trControl = trainControl(verboseIter = TRUE)
)
return(mod)
}
# Logistic Regression (Doesn't use my_train as it requires an extra argument)
mod_lr <- train(
x = train_x,
y = train_y,
method = 'glm',
family = 'binomial',
tuneLength = 5,
metric = 'kappa',
trControl = trainControl(verboseIter = TRUE)
)
train_x <- mod_x[train_rows, ]
test_x <- mod_x[-train_rows, ]
train_y <- mod_y[train_rows]
test_y <- mod_y[-train_rows]
# Algorithm Selection
#------------------------------------------------------------------------------
set.seed(314)
# Define custom train function
my_train <- function(method) {
mod <- train(
x = train_x,
y = train_y,
method = method,
tuneLength = 5,
metric = 'kappa',
trControl = trainControl(verboseIter = TRUE)
)
return(mod)
}
# Logistic Regression (Doesn't use my_train as it requires an extra argument)
mod_lr <- train(
x = train_x,
y = train_y,
method = 'glm',
family = 'binomial',
tuneLength = 5,
metric = 'kappa',
trControl = trainControl(verboseIter = TRUE)
)
warnings()
# Boosted Trees
mod_bt <- my_train('gbm')
# Random Forest
mod_rf <- my_train('ranger')
# Define custom train function
my_train <- function(method) {
mod <- train(
x = train_x,
y = train_y,
method = method,
tuneLength = 5,
metric = 'Kappa',
trControl = trainControl(verboseIter = TRUE)
)
return(mod)
}
prediction <- predict(mod_lr, test_x)
confusionMatrix(test_y, prediction)
true_positive <- sum(prediction == '1' & truth == '1')
truth <- test_y
true_positive <- sum(prediction == '1' & truth == '1')
false_positive <- sum(prediction == '1' & truth == '0')
?precision()
?F_meas()
# Define function to calculate f score
mod_list <- list(mod_lr, mod_bt, mod_rf)
mod_rf$modelType
mod_rf$method
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
})
)
View(mod_results)
mod_bt$times$everything
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = mod_bt$times$everything$elapsed
)
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = lapply(mod_list, function(i) i$times$everything$elapsed)
)
mod_lr$times$everything
mod_lr$times$everything['elapsed']
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = lapply(mod_list, function(i) i$times$everything['system'])
)
View(mod_results)
mod_lr$times$everything['system']
mod_lr$times$everything['user']
mod_lr$times$everything['elapsed']
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = lapply(mod_list, function(i) i$times$everything['elapsed'])
)
View(mod_results)
mod_results <- data_frame(
method = lapply(mod_list, function(i) i$method),
f = lapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = lapply(mod_list, function(i) unname(i$times$everything['elapsed']))
)
View(mod_results)
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = reorder(method, f), y = f)) +
geom_bar() +
coord_flip()
View(mod_results)
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = reorder(method, f + rnorm(1, 0, 0.001)), y = f)) +
geom_bar() +
coord_flip()
reorder(method, f)
View(mod_results)
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = method, y = f)) +
geom_bar() +
coord_flip()
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = method, y = f)) +
geom_col() +
coord_flip()
lapply(mod_list, function(i) i$method)
as.vector(lapply(mod_list, function(i) i$method))
sapply(mod_list, function(i) i$method)
mod_results <- data_frame(
method = sapply(mod_list, function(i) i$method),
f = sapply(mod_list, function(i) {
F_meas(predict(i, test_x), test_y)
}),
train_time = sapply(mod_list, function(i) {
unname(i$times$everything['elapsed'])
})
)
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = method, y = f)) +
geom_col() +
coord_flip()
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = reorder(method, f), y = f)) +
geom_col() +
coord_flip()
mod_results %>%
ggplot(aes(x = reorder(method, train_time), y = train_time)) +
geom_col() +
coord_flip()
# Plot best models by F score and training time
mod_results %>%
ggplot(aes(x = reorder(method, f), y = f)) +
geom_col() +
coord_flip() +
labs(x = 'method')
mod_results %>%
ggplot(aes(x = reorder(method, train_time), y = train_time)) +
geom_col() +
coord_flip() +
labs(x = 'method')
mod_results %>%
ggplot(aes(x = reorder(method, train_time), y = train_time)) +
geom_col() +
coord_flip() +
labs(x = 'method', title = 'Train Time by Model')
sort(rep(c('A', 'B', 'C', 'D'), 3))
knitr::opts_chunk$set(echo = TRUE)
df <- data.frame(
treatment = sort(rep(c('A', 'B', 'C', 'D'), 3)),
time_to_relief = c(14, 24, 12, 25, 20, 14, 17, 18, 22, 29, 36, 20)
)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
sort(rep(c('A', 'B', 'C'), 4))
View(df)
df <- data.frame(
treatment = sort(rep(c('A', 'B', 'C'), 4)),
time_to_relief = c(14, 24, 12, 25, 20, 14, 17, 18, 22, 29, 36, 20)
)
View(df)
?kable()
library(shiny)
library(magrittr)
library(tidyverse)
library(lubridate)
runApp('Documents/Misc/Shiny2019/Tracker_2019.R')
runApp('Documents/Misc/Shiny2019/Tracker_2019.R')
library(shiny); runApp('Documents/Git/shiny_apps/ShinyCalendar.R')
setwd('~/Documents/Git/shiny_apps')
x <- read_csv('www/MeData.csv')
x %<>% drop(FinanceBucket)
x %<>% select(-FinanceBucket)
x$Learning <- 0
x$Subject
x$Subject <- ''
x$LearningBucket <- 6
write.csv(x, 'www/MeData.csv', row.names = FALSE)
rm(x)
library(shiny)
library(magrittr)
library(tidyverse)
library(lubridate)
# Read in data
y2019 <- as.data.frame(read_csv('www/MeData.csv'))
for (i in seq_along(y2019)) {
if (str_detect(colnames(y2019)[i], 'Bucket')) {
y2019[,i] <- as.factor(y2019[,i])
}
}
# Read HTML
my_header <- readLines('www/calendar_header.txt')
# Create labels
month_labs <- format(ISOdate(2004, 1:12, 1), "%B")[1:month(today())]
cur_month <- month_labs[month(today())]
measure_labs <- c(
'Running', 'Music', 'Reading', 'Finances', 'Mental Health' = 'Mental',
'Eating Out' = 'Eating', 'Episodes Watched' = 'Episodes', 'Learning'
)
# Identify good and bad behaviors
good  <- c('Running', 'Music', 'Reading')
bad   <- c('Finances', 'Eating', 'Episodes')
# Define color schemes
seq_good <- c('#EDF8FB', '#B3CDE3', '#8C96C6', '#8856A7', '#810F7C', '#CCCCCC')
seq_bad  <- c('#FEE5D9', '#FCAE91', '#FB6A4A', '#DE2D26', '#A50F15', '#CCCCCC')
diverging<- c('#CA0020', '#F4A582', '#F7F7F7', '#92C5DE', '#0571B0', '#CCCCCC')
# Define cutoffs
cutoffs <- data_frame(
Running = c(2, 5, 7, 10),
Music = c(15, 30, 45, 60),
Reading = c(25, 50, 75, 100),
Finances = c(10, 25, 50, 100),
Mental = c(2, 4, 6, 8),
Eating = c(1, 2, 3, 4),
Episodes = c(3, 6, 9, 12)
)
# Define cutoffs
cutoffs <- data_frame(
Running = c(2, 5, 7, 10),
Music = c(15, 30, 45, 60),
Reading = c(25, 50, 75, 100),
Finances = c(10, 25, 50, 100),
Mental = c(2, 4, 6, 8),
Eating = c(1, 2, 3, 4),
Episodes = c(3, 6, 9, 12),
Learning = c(15, 30, 45, 60)
)
# Define functions
my_theme <- function() {
theme(
plot.background = element_rect(fill = '#FFFFFF', colour = '#FFFFFF'),
panel.grid = element_blank(),
panel.background = element_blank(),
axis.line = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank(),
axis.text = element_blank(),
legend.position = 'none'
)
}
yearly_plot <- function(col_scheme, bucket) {
y2019 %>%
ggplot(aes_string(x = 'Week', y = 'DayOfWeek', color = bucket)) +
geom_point(shape = 15, size = 4.5) +
scale_color_manual(values = col_scheme) +
my_theme()
}
monthly_plot <- function(selected_month, col_scheme, bucket, lab) {
y2019 %>%
filter(Month == selected_month) %>%
mutate(Week = max(Week) - Week + 1) %>%
ggplot(aes_string(x = 'DayOfWeek', y = 'Week', color = bucket)) +
geom_point(shape = 15, size = 45) +
geom_text(aes_string(label = lab), col = '#000000', size = 8) +
xlim(c(0.5, 7.5)) +
ylim(c(0.5, 5.5)) +
scale_color_manual(values = col_scheme) +
my_theme()
}
update_bucket <- function(input_value, cutoff_field)  {
if (input_value > max(cutoff_field)) {
result <- length(cutoff_field) + 1
} else {
result <- max(which(input_value <= cutoff_field)) + 1
}
return(result)
}
runApp('ShinyCalendar.R')
rm(list = ls())
runApp('TestScorePredictor.R')
